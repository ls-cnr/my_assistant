# Diario di Sviluppo: Assistente Personale con Avatar 3D - Parte XI

## Integrazione del Sistema di Sincronizzazione Labiale con Rhubarb

Dopo aver implementato il sistema di espressioni facciali e lo scheduler di animazione, sono passato all'integrazione della sincronizzazione labiale dell'avatar 3D. Questo componente è cruciale per rendere l'interazione con l'avatar più naturale e coinvolgente.
Le due principali opzioni considerate sono state Oculus LipSync e Rhubarb Lip Sync. Alla fine ho optato per Rhubarb Lip Sync.

### Motivazioni della Scelta
- **Oculus LipSync**: Il pacchetto Meta XR SDK è molto pesante (>50MB) e include numerose dipendenze non necessarie per il nostro caso d'uso specifico.
- **Rhubarb**: È una soluzione leggera che opera principalmente sul backend, con un'integrazione minimale richiesta sul frontend.

#### Altre considerazioni
- Rhubarb produce dati di sincronizzazione in formato JSON che possono essere facilmente trasmessi dal backend al frontend.
- Spostando l'analisi dei visemi sul backend, riduciamo il carico computazionale sul client Unity durante la riproduzione.
- Questo approccio è pensato per funzionare su dispositivi con risorse limitate rispetto all'analisi in tempo reale richiesta da Oculus LipSync.
- Rhubarb non è legato a specifiche piattaforme o ecosistemi, garantendo maggiore portabilità della soluzione (mentre Oculus LipSync è strettamente legato all'ecosistema Meta/VR, che non è il focus primario del nostro assistente).


### Architettura della Soluzione

Ho progettato un'architettura client-server per gestire la sincronizzazione labiale:

1. **Backend (Python)**:
   - Elabora l'audio e genera i dati di sincronizzazione labiale con Rhubarb
   - Invia l'audio e i dati di sincronizzazione al frontend Unity

2. **Frontend (Unity)**:
   - Riceve e memorizza i file audio e di sincronizzazione
   - Riproduce l'audio e anima la bocca dell'avatar in sincronia

### Componenti Principali

La soluzione è composta da due componenti principali:

1. **`RhubarbLipSyncManager`**: Gestisce la comunicazione HTTP tra il backend Python e Unity
   - Espone un server HTTP locale per ricevere i file
   - Gestisce l'upload e l'archiviazione di file audio e dati di sincronizzazione
   - Coordina le richieste di riproduzione e animazione
   - Configura il percorso di archiviazione dei file in base alle impostazioni nell'inspector

2. **`RhubarbLipSyncAnimation`**: Implementa l'animazione di sincronizzazione labiale
   - Estende `FaceAnimationItem` per integrarsi con il sistema di scheduling
   - Riproduce l'audio e anima la bocca dell'avatar in base ai dati di Rhubarb
   - Implementa una mappatura ottimizzata tra i visemi di Rhubarb e le blend shapes ARKit

#### 1. Integrazione con Unity Threads

La gestione della comunicazione HTTP in Unity ha rivelato un problema critico: le chiamate a `MonoBehaviour` non possono essere effettuate da thread secondari. Ho risolto questo problema implementando:

- Un listener HTTP in un thread separato
- Una coda di richieste elaborata nel thread principale
- Un sistema per spostare l'elaborazione delle richieste nel thread principale tramite il metodo `Update()`

#### 2. Trasferimento Efficiente dei File

Inizialmente, il trasferimento dei file audio tramite base64 in JSON causava problemi:
- URI troppo lunghi per Unity
- Prestazioni scadenti con file audio di dimensioni moderate

Ho riprogettato il sistema per utilizzare:
- Trasferimento diretto dei file binari tramite multipart/form-data
- Archiviazione locale dei file in percorsi configurabili
- Opzione per eliminare automaticamente i file dopo la riproduzione

#### 3. Strumenti di Supporto

Per facilitare il testing e l'uso del sistema, ho creato due script Python:

1. **`upload_to_unity.py`**: Carica file audio o do tipo JSON a Unity
   - Gestisce l'invio di file binari tramite multipart/form-data
   - Supporta la specificazione di nomi personalizzati
   - Fornisce feedback dettagliato sullo stato dell'upload

2. **`rhubarb_client.py`**: Richiede la riproduzione di file precedentemente caricati
   - Invia richieste GET all'endpoint di riproduzione
   - Semplifica il processo di test del sistema di sincronizzazione




### 3. Mappatura dei Visemi

La sincronizzazione del labiale ha richiesto di creare una mappatura tra i visemi di Rhubarb (A-H, X) e le blend shapes ARKit disponibili nell'avatar. Ho sviluppato una mappatura dettagliata basata su:

- Come i suoni del parlato corrispondono ai movimenti della bocca (vedi Rhubarb)
- Come le blend shapes ARKit possono essere combinate per creare espressioni naturali
- L'impatto dei valori di intensità sul realismo dell'animazione

La mappatura finale utilizza combinazioni di blend shapes per ciascun visema:


| Vis. Rhubarb | Descrizione | VisARkit1 | VisARkit2 | VisARkit3 | VisARkit4 | VisARkit5 | VisARkit6 |
|------------|-------------|-----------|-----------|-----------|-----------|-----------|-----------|
| Visema A | Bocca chiusa (P, B, M) | mouthClose (0.8f) | mouthPucker (0.1f) | | | | |
| Visema B | Bocca leggermente aperta (K, S, T, EE) | jawOpen (0.2f) | mouthClose (0.3f) | mouthStretchLeft (0.3f) | mouthStretchRight (0.3f) | | |
| Visema C | Bocca aperta (E, EH) | jawOpen (0.4f) | mouthClose (0.0f) | | | | |
| Visema D | Bocca molto aperta (AA) | jawOpen (0.7f) | mouthClose (0.0f) | | | | |
| Visema E | Bocca leggermente arrotondata (AO, ER) | jawOpen (0.3f) | mouthFunnel (0.4f) | | | | |
| Visema F | Labbra sporgenti (UW, OW, W) | jawOpen (0.2f) | mouthPucker (0.6f) | | | | |
| Visema G | Denti superiori su labbro inferiore (F, V) | jawOpen (0.3f) | mouthClose (0.1f) | mouthLowerDownLeft (0.5f) | mouthLowerDownRight (0.5f) | mouthPressLeft (0.3f) | mouthPressRight (0.3f) |
| Visema H | Lingua visibile (L prolungati) | jawOpen (0.5f) | tongueOut (0.3f) | | | | |
| Visema X | Posizione di riposo (bocca chiusa rilassata) | mouthClose (0.5f) | | | | | |

Al momento, il sistema di sincronizzazione labiale funziona, anche se l'animazione della bocca non è del tutto convincente. I valori precisi di intensità hanno un impatto significativo sul realismo dell'animazione, confermando l'osservazione precedente che piccole differenze nei valori possono produrre risultati visivamente molto diversi. Pertanto, i valori dovranno essere calibrati con maggiore precisione attraverso iterazioni di test specifiche per garantire dei movimenti più naturali.
